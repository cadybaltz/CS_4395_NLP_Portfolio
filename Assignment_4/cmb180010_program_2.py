"""
CS 4395.001 Human Language Technologies
Portfolio Chapter 8: Ngrams Program 2
Cady Baltz (cmb180010)
3/3/2023
"""

import os
import pickle
from nltk import word_tokenize
from nltk.util import ngrams

# Hardcoding constant values for file paths used in this program
INPUT_LANGUAGES = {
    'English': 'data/LangId.train.English',
    'French': 'data/LangId.train.French',
    'Italian': 'data/LangId.train.Italian'
}
TEST_FILE = 'data/LangId.test'
SOLUTION_FILE = 'data/LangId.sol'

def calculate_most_likely_language(line, unigram_dicts, bigram_dicts):
    """
      Processes a line of text by calculating its probability of being generated by each language with Laplace smoothing
      Returns the language with the highest probability
      Args:
          line: String of text to predict the language of
          unigram_dicts: Dictionary mapping languages to dictionaries containing their unigram counts
          bigram_dicts: Dictionary mapping languages to dictionaries containing their bigram counts
      Returns:
          most_likely_language: String containing the most likely language that "line" matches
      Example:
        >>> create_dictionaries("Je parle franÃ§ais")
        >>> French
    """

    # tokenize the current line
    tokens = word_tokenize(line)

    # use NLTK to create a bigrams list for the current line
    bigrams = list(ngrams(tokens, 2))

    # calculate the total vocabulary size by adding the lengths of the unigram dictionaries
    total_vocab_size = 0
    for dict in unigram_dicts.values():
        total_vocab_size += len(dict)

    # calculate the probability that each language generated the given line
    lang_probabilities = {}

    # initialize the probabilities to 1 for later calculations
    for language in bigram_dicts.keys():
        lang_probabilities[language] = 1

    # handle edge case of a single word line
    if len(tokens) == 1:
        bigrams.append(tokens[0] + ' ' + '.')

    # iterate through every bigram in the line
    for bigram in bigrams:
        current_bigram = bigram[0] + ' ' + bigram[1]
        current_unigram = bigram[0]

        # calculate the probability of this bigram in each language
        for language in bigram_dicts.keys():

            # check if the bigram is present in the current language's bigram dictionary
            if current_bigram in bigram_dicts[language]:
                # if so, find its count
                bigram_count = bigram_dicts[language][current_bigram]
            else:
                # otherwise, set the count to zero
                bigram_count = 0

            # check if the first word in the bigram is present in the current language's unigram dictionary
            if current_unigram in unigram_dicts[language]:
                # if so, find its unigram count
                unigram_count = unigram_dicts[language][current_unigram]
            else:
                # otherwise, set the count to zero
                unigram_count = 0

            # calculate the bigram's probability using Laplace smoothing to handle zero counts
            laplace_prob = (bigram_count + 1) / (unigram_count + total_vocab_size)

            # multiply this bigram's probability with the existing probability for that language
            lang_probabilities[language] *= laplace_prob

    # find the language with the maximum probability in the dictionary
    return max(lang_probabilities, key=lang_probabilities.get)


if __name__ == '__main__':
    """
      Makes language predictions for each line of a given test file using the unigram and bigram dictionaries
      that were created in cmb180010_program_1.py
      
      Writes results to a new file called predictions.txt and prints the accuracy and incorrect line numbers
      
      Args: None
      Returns: None
      Example:
        >>> Prediction accuracy: 0.97
        >>>
        >>> Lines numbers with incorrect predictions when compared with data/LangId.sol:
        >>> Line 24
        >>> Line 44
    """

    # Create a dictionary mapping languages to their unigram/bigram count dictionaries
    unigram_dicts = {}
    bigram_dicts = {}
    for lang in INPUT_LANGUAGES.items():

        # unpickle the files produced in program 1 (see "cmb180010_language_model.py")
        unigram_file_name = 'pickle_output/' + os.path.basename(lang[1]) + '_unigrams' + '.p'
        bigram_file_name = 'pickle_output/' + os.path.basename(lang[1]) + '_bigrams' + '.p'

        unigram_dicts[lang[0]] = pickle.load(open(unigram_file_name, 'rb'))
        bigram_dicts[lang[0]] = pickle.load(open(bigram_file_name, 'rb'))

    # open the file to make predictions for
    test_file = open(TEST_FILE, "r", encoding="utf8")
    test_lines = test_file.readlines()

    # create a file to output the most likely language for each line
    output_file = open("predictions.txt", "w")

    # compare the predictions made by the program to the solution file
    solution_file = open(SOLUTION_FILE, "r")
    solutions = solution_file.readlines()

    total_correct = 0
    incorrect_line_numbers = []

    for line_num in range(len(test_lines)):
        # find the language with the highest probability for each line in the test file
        predicted_result = calculate_most_likely_language(test_lines[line_num], unigram_dicts, bigram_dicts)

        # write this language to my output file
        output_file.write(str(line_num+1) + ' ' + predicted_result + '\n')

        # check whether the prediction was correct or incorrect
        actual_result = solutions[line_num].split()[1]
        if predicted_result != actual_result:
            incorrect_line_numbers.append(line_num)
        else:
            total_correct += 1

    # print results to the console
    print("Prediction accuracy: " + str(total_correct/len(test_lines)) + '\n')
    print("Lines numbers with incorrect predictions when compared with " + SOLUTION_FILE + ':')
    for num in incorrect_line_numbers:
        print("Line " + str(num+1))


