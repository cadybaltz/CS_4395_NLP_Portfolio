{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Portfolio Assignment: WordNet\n",
        "## Cady Baltz (cmb180010)\n",
        "## 2/26/2023"
      ],
      "metadata": {
        "id": "tfBKDh99mnZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Summary of WordNet\n",
        "\n",
        "WordNet is a lexical database of words that is organized hierarchically. Words are grouped with their synonyms into \"synsets\", which then form relationships with other synsets (such as part-whole relationships). This project was developed at Princeton University in the 1980s according to theories that humans organize concepts mentally in a hierarchy."
      ],
      "metadata": {
        "id": "Ch6NQJwrJMut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Select a noun and output all of its synsets"
      ],
      "metadata": {
        "id": "bzuBcHb0L2dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports and downloads required for using WordNet\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "pvFtYNoVKLUZ",
        "outputId": "cf3cf625-82a6-427a-96dc-20736849b40a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output all synsets (synonym sets) of my chosen noun (blade)\n",
        "\n",
        "print(\"All synsets for the word 'blade':\\n\")\n",
        "wn.synsets('blade')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LrJTWQmtJQen",
        "outputId": "a928100f-1385-45fb-dddb-9b2a249c9b1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All synsets for the word 'blade':\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('blade.n.01'),\n",
              " Synset('blade.n.02'),\n",
              " Synset('blade.n.03'),\n",
              " Synset('sword.n.01'),\n",
              " Synset('blade.n.05'),\n",
              " Synset('blade.n.06'),\n",
              " Synset('blade.n.07'),\n",
              " Synset('blade.n.08'),\n",
              " Synset('blade.n.09')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Analyze one synset from the list above"
      ],
      "metadata": {
        "id": "mUq-WpDyMDw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected one synset from the list outputted above\n",
        "blade_synset = wn.synset('blade.n.04')\n",
        "\n",
        "# Extract its definition/gloss\n",
        "print(\"Definition: \" + str(blade_synset.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2N-ZOLjtKUaj",
        "outputId": "2c31b344-d5c7-4c10-efc2-ded177d2171a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: a cutting or thrusting weapon that has a long metal blade and a hilt with a hand guard\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its usage examples (found none)\n",
        "print(\"Usage examples: \" + str(blade_synset.examples()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MZ5jTnFLKpGt",
        "outputId": "5e854429-ad0b-4803-c8b9-6c2c558e3e70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage examples: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its lemmas (WordNet entries that are synonyms in this set)\n",
        "lemmas = blade_synset.lemmas()\n",
        "print(\"Lemmas in the blade synset: \\n\")\n",
        "lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TJekxAQ4KrFw",
        "outputId": "ead312fd-14dd-4238-811b-e863699d53de"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmas in the blade synset: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('sword.n.01.sword'),\n",
              " Lemma('sword.n.01.blade'),\n",
              " Lemma('sword.n.01.brand'),\n",
              " Lemma('sword.n.01.steel')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traverse up the WordNet hierarchy (all the way up to entity.n.01), outputting synsets as you go\n",
        "\n",
        "# Create a function \"hyper\" that returns the hypernyms of a synset\n",
        "hyper = lambda s: s.hypernyms()\n",
        "\n",
        "# Use the NLTK closure method to create a list of synsets for each level of hypernym\n",
        "print(\"Traversing up the WordNet Hierarchy from blade.n.04 to entity.n.01:\\n\")\n",
        "list(blade_synset.closure(hyper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bjr_IgSmu7MG",
        "outputId": "501a6985-841f-4843-f185-978d8dbb33b0"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traversing up the WordNet Hierarchy from blade.n.04 to entity.n.01:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('weapon.n.01'),\n",
              " Synset('instrument.n.01'),\n",
              " Synset('device.n.01'),\n",
              " Synset('instrumentality.n.03'),\n",
              " Synset('artifact.n.01'),\n",
              " Synset('whole.n.02'),\n",
              " Synset('object.n.01'),\n",
              " Synset('physical_entity.n.01'),\n",
              " Synset('entity.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For nouns, the hierarchical structure of WordNet dictates that they all fall under one large synset, 'entity.n.01'. As you can see from the previous output, nouns are then divided into more and more specific groups. Overall, for the word 'blade', these divisions were very logical and helped narrow down the synset to 'weapon' right before 'blade', which would be a useful classification for this word."
      ],
      "metadata": {
        "id": "l0rFRwOXxMPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Additional information about the synset's hierarchical relationships"
      ],
      "metadata": {
        "id": "hVg-MiDUMd1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its hypernyms (higher synsets than the given one)\n",
        "print(\"Hypernyms:\\n\")\n",
        "\n",
        "# Prints a more general synset for a blade (weapon)\n",
        "blade_synset.hypernyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nUQQ6u22Mys0",
        "outputId": "43d9580c-d186-4089-c80b-4531189a408a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hypernyms:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('weapon.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its hyponyms (lower synsets than the given one)\n",
        "print(\"Hyponyms:\\n\")\n",
        "\n",
        "# Prints a list of more specific types of blades, such as broadsword\n",
        "blade_synset.hyponyms()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Xw2xuzEMM3cH",
        "outputId": "0d9b4435-927a-42b2-d30e-0542202061c5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyponyms:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('backsword.n.02'),\n",
              " Synset('broadsword.n.01'),\n",
              " Synset('cavalry_sword.n.01'),\n",
              " Synset('cutlas.n.01'),\n",
              " Synset('falchion.n.01'),\n",
              " Synset('fencing_sword.n.01'),\n",
              " Synset('rapier.n.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its meronyms (synsets that are part of the given one)\n",
        "\n",
        "# Found none\n",
        "print(\"Member meronyms: \" + str(blade_synset.member_meronyms()))\n",
        "\n",
        "# Prints words that describe different \"parts\" of a blade, like a blade's hilt\n",
        "print(\"Part meronyms: \" + str(blade_synset.part_meronyms()))\n",
        "\n",
        "# Found none\n",
        "print(\"Substance meronyms: \" + str(blade_synset.substance_meronyms()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rO-11qZXM5Nl",
        "outputId": "2c0c7b2d-f036-4e84-fb51-97c06ad9ec98"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Member meronyms: []\n",
            "Part meronyms: [Synset('blade.n.09'), Synset('foible.n.02'), Synset('forte.n.03'), Synset('haft.n.01'), Synset('hilt.n.01'), Synset('point.n.08')]\n",
            "Substance meronyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its holonyms (synsets that make up the whole of which the current synset is a part of)\n",
        "\n",
        "# Found none\n",
        "print(\"Member meronyms: \" + str(blade_synset.member_holonyms()))\n",
        "\n",
        "# Found none\n",
        "print(\"Part meronyms: \" + str(blade_synset.part_holonyms()))\n",
        "\n",
        "# Found none\n",
        "print(\"Substance meronyms: \" + str(blade_synset.substance_holonyms()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RCGrFUNwNmma",
        "outputId": "930bee0f-8146-4085-94b1-767f024b0a32"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Member meronyms: []\n",
            "Part meronyms: []\n",
            "Substance meronyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its antonyms\n",
        "\n",
        "# WordNet finds antonyms for specific lemmas, as opposed to synsets\n",
        "for lemma in lemmas:\n",
        "  \n",
        "  # No antonyms were found for any of blade's lemmas\n",
        "  print(\"Antonyms for \" + str(lemma) + \": \" + str(lemma.antonyms()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oiYUE9-jN3xY",
        "outputId": "c1d8835b-3ce7-4717-b69a-da867a6ff2cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antonyms for Lemma('sword.n.01.sword'): []\n",
            "Antonyms for Lemma('sword.n.01.blade'): []\n",
            "Antonyms for Lemma('sword.n.01.brand'): []\n",
            "Antonyms for Lemma('sword.n.01.steel'): []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Select a verb and output all synsets"
      ],
      "metadata": {
        "id": "PjMwtHLaOT_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output all synsets of my chosen verb\n",
        "\n",
        "print(\"All synsets for the verb 'shatter':\\n\")\n",
        "wn.synsets('shatter')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b4YuyKH-1Ltl",
        "outputId": "c24bd4da-57bc-402d-a1d2-10b9d9d51afb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All synsets for the verb 'shatter':\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('shatter.v.01'), Synset('shatter.v.02'), Synset('shatter.v.03')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Analyze one synset from the list above"
      ],
      "metadata": {
        "id": "hbYZ5hWI1UI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Selected one synset from the list outputted above\n",
        "shatter_synset = wn.synset('shatter.v.01')\n",
        "\n",
        "# Extract its definition/gloss\n",
        "print(\"Definition: \" + str(shatter_synset.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F_vmNUuq1SXs",
        "outputId": "0f7e051a-e726-47ce-c1f8-b2b5487f59ba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: break into many pieces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract usage examples for the verb shatter\n",
        "print(\"Usage examples: \" + str(shatter_synset.examples()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_z1keGz71phb",
        "outputId": "c5cee1ca-2536-4b95-dc4a-4d9b06c5acbd"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage examples: ['The wine glass shattered']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract its lemmas\n",
        "lemmas = shatter_synset.lemmas()\n",
        "print(\"Lemmas in the shatter synset: \\n\")\n",
        "lemmas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HeOCJ9sf1utr",
        "outputId": "ca2535b8-7132-406a-f59d-9e687bba72a0"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmas in the shatter synset: \n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('shatter.v.01.shatter')]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traverse up the WordNet hierarchy, outputting synsets as you go\n",
        "\n",
        "# Create a function \"hyper\" that returns the hypernyms of a synset\n",
        "hyper = lambda s: s.hypernyms()\n",
        "\n",
        "# Use the NLTK closure method to create a list of synsets for each level of hypernym\n",
        "print(\"Traversing up the WordNet Hierarchy for shatter:\\n\")\n",
        "list(shatter_synset.closure(hyper))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5AQVcWbO12Ad",
        "outputId": "32a127ab-a738-4319-d11b-352cd2a5607b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traversing up the WordNet Hierarchy for shatter:\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Synset('burst.v.08'),\n",
              " Synset('break.v.02'),\n",
              " Synset('change_integrity.v.01'),\n",
              " Synset('change.v.02')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In comparison with nouns, I found that verbs were less logically well-organized in WordNet. When I traversed up the hierarchy for 'shatter', I reached a final synset of 'change', while all nouns arrive at the same final synset. Additionally, my verb did not have as many layers of hypernyms as my noun did, and they were less descriptive overall."
      ],
      "metadata": {
        "id": "bv2nY_wIpB76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Use Morphy to find as many different forms of the word as you can"
      ],
      "metadata": {
        "id": "ix4yiTWyE7Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyzing both words from earlier ('blade' and 'shatter') with Morphy\n",
        "\n",
        "for word in ['blade', 'shatter']:\n",
        "  print(\"Different forms of the word '\" + word + \"':\")\n",
        "  print(\"Adjective form: \" + str(wn.morphy(word, wn.ADJ)))\n",
        "  print(\"Verb form: \" + str(wn.morphy(word, wn.VERB)))\n",
        "  print(\"Adverb form: \" + str(wn.morphy(word, wn.ADV)))\n",
        "  print(\"Noun form: \" + str(wn.morphy(word, wn.NOUN)))\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LPPEc7TnE_RR",
        "outputId": "52e16cb2-35b3-4b00-f7fc-72cae8021d90"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Different forms of the word 'blade':\n",
            "Adjective form: None\n",
            "Verb form: None\n",
            "Adverb form: None\n",
            "Noun form: blade\n",
            "\n",
            "Different forms of the word 'shatter':\n",
            "Adjective form: None\n",
            "Verb form: shatter\n",
            "Adverb form: None\n",
            "Noun form: None\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Compare two words that I think might be similar"
      ],
      "metadata": {
        "id": "a60jxISNHBMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the specific synsets I am interested in\n",
        "\n",
        "# First, I printed all of the synsets\n",
        "print(\"All synsets for the word 'happiness':\")\n",
        "print(wn.synsets('happiness'))\n",
        "print()\n",
        "print(\"All synsets for the word 'joy':\")\n",
        "print(wn.synsets('joy'))\n",
        "print()\n",
        "\n",
        "# After analyzing the definitions of each synset, I chose the following two synsets for comparison\n",
        "\n",
        "happiness_synset = wn.synset('happiness.n.01')\n",
        "print(\"'happiness' definition: \" + str(happiness_synset.definition()))\n",
        "\n",
        "joy_synset = wn.synset('joy.n.01')\n",
        "print(\"'joy' definition: \" + str(joy_synset.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nDwm66kKHMbM",
        "outputId": "7f8fa018-e244-4baf-ad37-1312e118021e"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All synsets for the word 'happiness':\n",
            "[Synset('happiness.n.01'), Synset('happiness.n.02')]\n",
            "\n",
            "All synsets for the word 'joy':\n",
            "[Synset('joy.n.01'), Synset('joy.n.02'), Synset('rejoice.v.01'), Synset('gladden.v.01')]\n",
            "\n",
            "'happiness' definition: state of well-being characterized by emotions ranging from contentment to intense joy\n",
            "'joy' definition: the emotion of great happiness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Wu-Palmer similarity metric\n",
        "\n",
        "print(\"Similarity between 'happiness' and 'joy' according to Wu-Palmer Similarity Metric:\")\n",
        "print(wn.wup_similarity(happiness_synset, joy_synset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hE-VRFazHbwF",
        "outputId": "de4ac2e0-a696-42fc-b5e2-f173a286006d"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between 'happiness' and 'joy' according to Wu-Palmer Similarity Metric:\n",
            "0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Lesk algorithm\n",
        "\n",
        "from nltk.wsd import lesk\n",
        "sent = ['She', 'felt', 'a', 'sense', 'of', 'happiness', 'at', 'the', 'party', '.']\n",
        "\n",
        "result = lesk(sent, 'happiness', 'n')\n",
        "print(\"Lesk algorithm result for 'happiness':\")\n",
        "print(str(result) + ': ' + str(result.definition()))\n",
        "print()\n",
        "\n",
        "sent[5] = 'joy'\n",
        "result = lesk(sent, 'joy', 'n')\n",
        "print(\"Lesk algorithm result for 'joy':\")\n",
        "print(str(result) + ': ' + str(result.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0BtxEzI5JFoM",
        "outputId": "90507679-f812-4cb3-dff7-ac3ec34f7208"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lesk algorithm result for 'happiness':\n",
            "Synset('happiness.n.02'): emotions experienced when in a state of well-being\n",
            "\n",
            "Lesk algorithm result for 'joy':\n",
            "Synset('joy.n.02'): something or someone that provides a source of happiness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Wu-Palmer similarity metric seemed to be fairly accurate with these two words, as it said that 'joy' and 'happiness' were 80% similar. When analyzing these words with the Lesk algorithm, they were also fairly similar, with the output for 'joy' even containing the word 'happiness'. The main difference is that Lesk indicated that 'happiness' is an emotion, while 'joy' is something that causes the emotion."
      ],
      "metadata": {
        "id": "oedgI8ePK4iC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Analysis with SentiWordNet"
      ],
      "metadata": {
        "id": "lXfgsGb9LBlt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SentiWordNet is a resource that assigns sentiment scores to synsets. Sentiment scores indicate how likely it is that lemmas in this synset are being used positively, negatively, or objectively. SentiWordNet has many possible use cases in the real world today. For example, it would be useful in analyzing the emotion displayed in a tweet to determine how a user feels about a certain topic."
      ],
      "metadata": {
        "id": "IQDfgoAaLGiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports for SentiWordNet\n",
        "nltk.download('sentiwordnet')\n",
        "from nltk.corpus import sentiwordnet as swn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hVT6ci4yK_kP",
        "outputId": "d0428417-fe0d-4864-cc95-d062b1821ebe"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select an emotionally charged word and find its senti-synsets\n",
        "senti_synsets = swn.senti_synsets('vicious')\n",
        "print(\"Senti-synsets for the word 'vicious':\")\n",
        "\n",
        "# Output the polarity scores for each synset\n",
        "for synset in senti_synsets:\n",
        "  print()\n",
        "  print(synset.synset)\n",
        "  print(\"Positive score: \", synset.pos_score())\n",
        "  print(\"Negative score: \", synset.neg_score())\n",
        "  print(\"Objective score: \", synset.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-5q8rC81LLvW",
        "outputId": "e2f6b50f-e010-456e-8f92-ed6f3ce3905d"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Senti-synsets for the word 'vicious':\n",
            "\n",
            "Synset('barbarous.s.01')\n",
            "Positive score:  0.0\n",
            "Negative score:  0.625\n",
            "Objective score:  0.375\n",
            "\n",
            "Synset('evil.s.02')\n",
            "Positive score:  0.0\n",
            "Negative score:  0.5\n",
            "Objective score:  0.5\n",
            "\n",
            "Synset('condemnable.s.01')\n",
            "Positive score:  0.0\n",
            "Negative score:  0.875\n",
            "Objective score:  0.125\n",
            "\n",
            "Synset('poisonous.s.03')\n",
            "Positive score:  0.0\n",
            "Negative score:  0.75\n",
            "Objective score:  0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make up a sentence\n",
        "sentence = 'NLP is such an interesting and exciting course'\n",
        "\n",
        "# Output the polarity for each word in the sentence\n",
        "tokens = sentence.split()\n",
        "\n",
        "neg = 0\n",
        "pos = 0\n",
        "\n",
        "for token in tokens:\n",
        "  syn_list = list(swn.senti_synsets(token))\n",
        "  if syn_list:\n",
        "    syn = syn_list[0]\n",
        "    print(\"Polarity scores for the word '\" + token + \"'\")\n",
        "    print(\"Positive score: \", syn.pos_score())\n",
        "    print(\"Negative score: \", syn.neg_score())\n",
        "    print(\"Objective score: \", syn.obj_score())\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LCFOlaNxL66m",
        "outputId": "64cc490f-8be9-45d8-88b6-3e5f9eeaa0f2"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity scores for the word 'NLP'\n",
            "Positive score:  0.0\n",
            "Negative score:  0.0\n",
            "Objective score:  1.0\n",
            "\n",
            "Polarity scores for the word 'is'\n",
            "Positive score:  0.25\n",
            "Negative score:  0.125\n",
            "Objective score:  0.625\n",
            "\n",
            "Polarity scores for the word 'such'\n",
            "Positive score:  0.0\n",
            "Negative score:  0.125\n",
            "Objective score:  0.875\n",
            "\n",
            "Polarity scores for the word 'an'\n",
            "Positive score:  0.0\n",
            "Negative score:  0.125\n",
            "Objective score:  0.875\n",
            "\n",
            "Polarity scores for the word 'interesting'\n",
            "Positive score:  0.125\n",
            "Negative score:  0.0\n",
            "Objective score:  0.875\n",
            "\n",
            "Polarity scores for the word 'exciting'\n",
            "Positive score:  0.25\n",
            "Negative score:  0.375\n",
            "Objective score:  0.375\n",
            "\n",
            "Polarity scores for the word 'course'\n",
            "Positive score:  0.0\n",
            "Negative score:  0.0\n",
            "Objective score:  1.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, the output from SentiWordNet seemed accurate, but conservative. For example, SentiWordNet indicated that the word 'evil' is 50% likely to be negative and 50% likely to be objective. If I were to have chosen these numbers myself, I would have made it more likely to be negative, so it seems like SentiWordNet prefers to classify something as objective rather than making mistakes. From my sentence input, I observed that nouns are more likely to be scored as 100% objective than verbs and adjectives. I think that these scores would be useful in analyzing emotional texts, but they are ultimately limited by the conservative nature of the algorithm. For texts that are only slightly positive or slightly negative, the tool may not be that helpful in making a classification."
      ],
      "metadata": {
        "id": "RzM6LiWLOTt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Collocations"
      ],
      "metadata": {
        "id": "185A30x3OcQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collocations are formed when two or more words occur together with a significant frequency, such as 'vice president'. It is important to know which words are part of a collocation, because you cannot logically substitute synonyms within them (e.g. 'break the ice' does not mean the same thing as 'shatter the ice'). For NLP applications like language translation, extracting collocations and processing them as one unit is essential and is often solved using frequency analysis."
      ],
      "metadata": {
        "id": "lnDtK9CxObVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary downloads for accessing text4 (the Inaugural corpus)\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eZ57QPMXPCu0",
        "outputId": "70a64a34-a9a9-4c73-e913-b5c52c63dc1d"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output collocations for text4, the Inaugural corpus\n",
        "from nltk.book import text4\n",
        "\n",
        "text4.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5gxN_XXMOf_u",
        "outputId": "c21724e1-b66e-40ba-d6a5-8446f27e8bdf"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select one of the collocations identified by NLTK\n",
        "collocation = 'every citizen'\n",
        "\n",
        "# Calculate mutual information using the formula log(P(x,y) / [P(x) * P(y)])\n",
        "import math\n",
        "\n",
        "text = ' '.join(text4.tokens)\n",
        "vocab = len(set(text4))\n",
        "\n",
        "# Calculate P(x,y)\n",
        "Pxy = text.count(collocation) / vocab\n",
        "print(\"p(every citizen) = \", Pxy)\n",
        "\n",
        "# Calculate P(x)\n",
        "Px = text.count(collocation.split()[0]) / vocab\n",
        "print(\"p(every) = \", Px)\n",
        "\n",
        "# Calculate P(y)\n",
        "Py = text.count(collocation.split()[1]) / vocab\n",
        "print('p(citizen) = ', Py)\n",
        "\n",
        "print()\n",
        "\n",
        "# Plug the above values into the final formula to get the result\n",
        "mutual_info = math.log2(Pxy / (Px * Py))\n",
        "print(\"Mutual information of 'every citizen' = \", mutual_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "REtcb2QpPbkH",
        "outputId": "dd99506b-2c66-44fc-ac6e-a1301693edee"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(every citizen) =  0.0016957605985037406\n",
            "p(every) =  0.03291770573566085\n",
            "p(citizen) =  0.032618453865336655\n",
            "\n",
            "Mutual information of 'every citizen' =  0.6593084177360863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the mutual information formula, we received a positive result of 0.6, indicating that these two words do form a collocation. However, this value is close to zero, so it is not a very strong determination. I would agree with this output, as 'every citizen' is not as common as a phrase like 'public debt', but I can understand why these words would frequently appear together in the Inaugural corpus."
      ],
      "metadata": {
        "id": "5MTkzpzAu6vO"
      }
    }
  ]
}